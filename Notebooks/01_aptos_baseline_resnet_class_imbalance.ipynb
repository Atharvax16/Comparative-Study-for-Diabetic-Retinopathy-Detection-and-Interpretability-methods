{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWmveR5E-sZC",
        "outputId": "af3de049-20d0-43ca-a6bf-04124c4df6ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-skinny==3.8.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.8.1 (from mlflow)\n",
            "  Downloading mlflow_tracing-3.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting Flask-CORS<7 (from mlflow)\n",
            "  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huey<3,>=2.5.0 (from mlflow)\n",
            "  Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.45)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.8.1->mlflow)\n",
            "  Downloading databricks_sdk-0.78.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (0.123.10)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.46)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (8.7.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (0.5.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (0.40.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.5)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow) (0.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (2026.1.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.8.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow) (4.12.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.8.1-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.8.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huey-2.6.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.78.0-py3-none-any.whl (780 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: huey, gunicorn, graphql-core, graphql-relay, docker, graphene, Flask-CORS, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
            "Successfully installed Flask-CORS-6.0.2 databricks-sdk-0.78.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 huey-2.6.0 mlflow-3.8.1 mlflow-skinny-3.8.1 mlflow-tracing-3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9K_Sdrkd5-rU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader,WeightedRandomSampler\n",
        "from torchvision import transforms,models\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,recall_score,precision_score,f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO8Avymq_Fjh",
        "outputId": "2b2387c9-bf9c-44b6-a733-40e47f49d931"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "APTOS_ROOT = \"/content/drive/MyDrive/Diabetic_retinopathy/data/APTOS\"\n",
        "\n",
        "TRAIN_IMG_DIR = f\"{APTOS_ROOT}/train_images\"\n",
        "VAL_IMG_DIR   = f\"{APTOS_ROOT}/val_images\"\n",
        "\n",
        "TRAIN_CSV = f\"{APTOS_ROOT}/train_1.csv\"\n",
        "VAL_CSV   = f\"{APTOS_ROOT}/valid.csv\""
      ],
      "metadata": {
        "id": "HofaRryb6TzL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 5\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "VAL_BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "SEED = 42\n"
      ],
      "metadata": {
        "id": "IqCVSDe8BM-k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"resnet50\"   # options: \"resnet50\", \"resnet18\"\n",
        "LOSS_TYPE = \"weighted_ce\" # options: \"ce\", \"weighted_ce\", \"focal\"\n",
        "USE_SAMPLER = False       # True/False\n",
        "FOCAL_GAMMA = 2.0"
      ],
      "metadata": {
        "id": "U7mDofH1BQXq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_NAME = \"APTOS_Imbalance_Experiments\""
      ],
      "metadata": {
        "id": "G60Wifku6aBq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPZZAq7v6cph",
        "outputId": "ce00cf7b-4024-4c2a-ae94-24c8c46c6c19"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(TRAIN_CSV)\n",
        "val_data   = pd.read_csv(VAL_CSV)\n",
        "\n",
        "train_data.columns = train_data.columns.str.strip()\n",
        "val_data.columns   = val_data.columns.str.strip()"
      ],
      "metadata": {
        "id": "IgMyS5Fb6dsm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert \"id_code\" in train_data.columns and \"diagnosis\" in train_data.columns, \"CSV must have id_code, diagnosis\"\n",
        "assert \"id_code\" in val_data.columns and \"diagnosis\" in val_data.columns, \"CSV must have id_code, diagnosis\"\n",
        "\n",
        "print(\"Train rows:\", len(train_data), \"Val rows:\", len(val_data))\n",
        "print(\"Train class counts:\\n\", train_data[\"diagnosis\"].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm57oCRD6gWr",
        "outputId": "6c102010-9c88-4cea-b77a-7ff660de4033"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rows: 2930 Val rows: 366\n",
            "Train class counts:\n",
            " diagnosis\n",
            "0    1434\n",
            "1     300\n",
            "2     808\n",
            "3     154\n",
            "4     234\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n"
      ],
      "metadata": {
        "id": "4iyYU8Hr6h4x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _resolve_path(self, img_id: str):\n",
        "        img_id = str(img_id)\n",
        "\n",
        "        # if csv has extension already\n",
        "        if img_id.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            p = os.path.join(self.img_dir, img_id)\n",
        "            if os.path.exists(p):\n",
        "                return p\n",
        "\n",
        "        for ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
        "            p = os.path.join(self.img_dir, img_id + ext)\n",
        "            if os.path.exists(p):\n",
        "                return p\n",
        "\n",
        "        return None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_id = row[\"id_code\"]\n",
        "        label = int(row[\"diagnosis\"])\n",
        "\n",
        "        img_path = self._resolve_path(img_id)\n",
        "        if img_path is None:\n",
        "            raise FileNotFoundError(\n",
        "                f\"Missing image for id_code='{img_id}' in '{self.img_dir}'. \"\n",
        "                f\"Checked png/jpg/jpeg.\"\n",
        "            )\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "SEN9y-2h6jNx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = APTOSDataset(train_data, TRAIN_IMG_DIR, train_tfms)\n",
        "val_ds   = APTOSDataset(val_data, VAL_IMG_DIR, val_tfms)"
      ],
      "metadata": {
        "id": "9N0tT2yR7rNN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_train_loader(use_sampler: bool):\n",
        "    if not use_sampler:\n",
        "        return DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=torch.cuda.is_available(),\n",
        "        )\n",
        "\n",
        "    labels = train_df[\"diagnosis\"].values\n",
        "    class_count = np.bincount(labels, minlength=NUM_CLASSES)\n",
        "    class_weights = 1.0 / np.maximum(class_count, 1)\n",
        "    sample_weights = class_weights[labels]\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=torch.DoubleTensor(sample_weights),\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    return DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=sampler,\n",
        "        num_workers=2,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "train_loader = build_train_loader(USE_SAMPLER)\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=VAL_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available(),\n",
        ")"
      ],
      "metadata": {
        "id": "VCTzUMvq_zHX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(name: str):\n",
        "    if name == \"resnet50\":\n",
        "        m = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        m.fc = nn.Linear(m.fc.in_features, NUM_CLASSES)\n",
        "        return m\n",
        "    if name == \"resnet18\":\n",
        "        m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        m.fc = nn.Linear(m.fc.in_features, NUM_CLASSES)\n",
        "        return m\n",
        "    raise ValueError(\"Unknown MODEL_NAME\")\n",
        "\n",
        "model = build_model(MODEL_NAME).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiA2ysFO_5Vg",
        "outputId": "f024b60c-7cae-4aec-9a62-b970bb559f7e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 166MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = train_data[\"diagnosis\"].value_counts().sort_index().reindex(range(NUM_CLASSES), fill_value=0)\n",
        "weights = counts.sum() / (NUM_CLASSES * np.maximum(counts.values, 1))\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "glm9-xjH_8LP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, weight=self.alpha, reduction=\"none\")\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "fvbA-R4J_-6L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOSS_TYPE == \"ce\":\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "elif LOSS_TYPE == \"weighted_ce\":\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "elif LOSS_TYPE == \"focal\":\n",
        "    criterion = FocalLoss(alpha=class_weights, gamma=FOCAL_GAMMA)\n",
        "else:\n",
        "    raise ValueError(\"LOSS_TYPE must be one of: ce, weighted_ce, focal\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
      ],
      "metadata": {
        "id": "FDuBOKAfADVh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, loader):\n",
        "    model.eval()\n",
        "    all_labels, all_probs, all_preds = [], [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "            preds = np.argmax(probs, axis=1)\n",
        "\n",
        "            all_probs.append(probs)\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovr\")\n",
        "    except Exception:\n",
        "        auc = None\n",
        "\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    weighted_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "    # per-class recall (especially for class 3 & 4)\n",
        "    recalls = recall_score(all_labels, all_preds, average=None, labels=list(range(NUM_CLASSES)))\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        \"loss\": avg_loss,\n",
        "        \"acc\": acc,\n",
        "        \"auc\": auc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"weighted_f1\": weighted_f1,\n",
        "        \"recalls\": recalls,\n",
        "        \"cm\": cm,\n",
        "        \"labels\": all_labels,\n",
        "        \"preds\": all_preds,\n",
        "    }"
      ],
      "metadata": {
        "id": "c0FW0gUWAGE9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "\n",
        "    return total_loss / len(loader.dataset)"
      ],
      "metadata": {
        "id": "qJUEG1seAK6W"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
        "    fig = plt.figure(figsize=(6, 5))\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.colorbar()\n",
        "    plt.xticks(range(NUM_CLASSES))\n",
        "    plt.yticks(range(NUM_CLASSES))\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ],
      "metadata": {
        "id": "HWuNjzmFANPp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "\n",
        "run_name = f\"{MODEL_NAME}_{LOSS_TYPE}_sampler{USE_SAMPLER}_img{IMG_SIZE}_lr{LR}\"\n",
        "with mlflow.start_run(run_name=run_name):\n",
        "\n",
        "    # log params\n",
        "    mlflow.log_param(\"model_name\", MODEL_NAME)\n",
        "    mlflow.log_param(\"img_size\", IMG_SIZE)\n",
        "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
        "    mlflow.log_param(\"epochs\", EPOCHS)\n",
        "    mlflow.log_param(\"lr\", LR)\n",
        "    mlflow.log_param(\"weight_decay\", WEIGHT_DECAY)\n",
        "    mlflow.log_param(\"loss_type\", LOSS_TYPE)\n",
        "    mlflow.log_param(\"use_sampler\", USE_SAMPLER)\n",
        "    mlflow.log_param(\"focal_gamma\", FOCAL_GAMMA if LOSS_TYPE == \"focal\" else None)\n",
        "    mlflow.log_param(\"seed\", SEED)\n",
        "    mlflow.log_param(\"device\", device)\n",
        "\n",
        "    # log class weights for reproducibility\n",
        "    for c in range(NUM_CLASSES):\n",
        "        mlflow.log_param(f\"class_weight_{c}\", float(class_weights[c].detach().cpu()))\n",
        "\n",
        "    best_val_auc = -1\n",
        "    best_ckpt_path = \"best_model.pt\"\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr_loss = train_one_epoch(model, train_loader)\n",
        "        val_out = eval_model(model, val_loader)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
        "        print(f\"Train loss: {tr_loss:.4f}\")\n",
        "        print(f\"Val loss:   {val_out['loss']:.4f}\")\n",
        "        print(f\"Val acc:    {val_out['acc']:.4f}\")\n",
        "        print(f\"Val auc:    {val_out['auc']}\")\n",
        "        print(f\"Val macroF1:{val_out['macro_f1']:.4f}\")\n",
        "        print(\"Val recalls:\", val_out[\"recalls\"])\n",
        "        print(\"Val CM:\\n\", val_out[\"cm\"])\n",
        "\n",
        "        # log metrics per epoch\n",
        "        mlflow.log_metric(\"train_loss\", tr_loss, step=epoch)\n",
        "        mlflow.log_metric(\"val_loss\", val_out[\"loss\"], step=epoch)\n",
        "        mlflow.log_metric(\"val_acc\", val_out[\"acc\"], step=epoch)\n",
        "        if val_out[\"auc\"] is not None:\n",
        "            mlflow.log_metric(\"val_auc\", float(val_out[\"auc\"]), step=epoch)\n",
        "        mlflow.log_metric(\"val_macro_f1\", val_out[\"macro_f1\"], step=epoch)\n",
        "        mlflow.log_metric(\"val_weighted_f1\", val_out[\"weighted_f1\"], step=epoch)\n",
        "\n",
        "        # log per-class recall\n",
        "        for c in range(NUM_CLASSES):\n",
        "            mlflow.log_metric(f\"val_recall_class{c}\", float(val_out[\"recalls\"][c]), step=epoch)\n",
        "\n",
        "        # save best by AUC (or fallback acc if auc None)\n",
        "        current_score = float(val_out[\"auc\"]) if val_out[\"auc\"] is not None else float(val_out[\"acc\"])\n",
        "        if current_score > best_val_auc:\n",
        "            best_val_auc = current_score\n",
        "            torch.save(model.state_dict(), best_ckpt_path)\n",
        "\n",
        "    # Final evaluation + artifacts from best checkpoint\n",
        "    model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "    final_out = eval_model(model, val_loader)\n",
        "\n",
        "    # Log final metrics\n",
        "    mlflow.log_metric(\"best_val_score\", best_val_auc)\n",
        "    mlflow.log_metric(\"final_val_acc\", final_out[\"acc\"])\n",
        "    if final_out[\"auc\"] is not None:\n",
        "        mlflow.log_metric(\"final_val_auc\", float(final_out[\"auc\"]))\n",
        "    mlflow.log_metric(\"final_val_macro_f1\", final_out[\"macro_f1\"])\n",
        "    mlflow.log_metric(\"final_val_weighted_f1\", final_out[\"weighted_f1\"])\n",
        "\n",
        "    # Save & log confusion matrix image\n",
        "    cm_fig = plot_confusion_matrix(final_out[\"cm\"], title=f\"CM_{run_name}\")\n",
        "    cm_path = \"confusion_matrix.png\"\n",
        "    cm_fig.savefig(cm_path, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close(cm_fig)\n",
        "    mlflow.log_artifact(cm_path)\n",
        "\n",
        "    # Save & log classification report\n",
        "    report = classification_report(final_out[\"labels\"], final_out[\"preds\"], digits=4)\n",
        "    report_path = \"classification_report.txt\"\n",
        "    with open(report_path, \"w\") as f:\n",
        "        f.write(report)\n",
        "    mlflow.log_artifact(report_path)\n",
        "\n",
        "    # Log model as MLflow artifact + raw checkpoint\n",
        "    mlflow.pytorch.log_model(model, artifact_path=\"model\")\n",
        "    mlflow.log_artifact(best_ckpt_path)\n",
        "\n",
        "print(\"Done. Check MLflow runs (mlruns folder or MLflow UI).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf_jDDwbAPVL",
        "outputId": "771fbd92-36db-45fb-8f20-b559ff8b9b24"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Train loss: 1.0532\n",
            "Val loss:   0.9536\n",
            "Val acc:    0.7240\n",
            "Val auc:    0.907668723295512\n",
            "Val macroF1:0.5835\n",
            "Val recalls: [0.98255814 0.65       0.38461538 0.72727273 0.5       ]\n",
            "Val CM:\n",
            " [[169   3   0   0   0]\n",
            " [  0  26   7   4   3]\n",
            " [  3  14  40  32  15]\n",
            " [  0   1   0  16   5]\n",
            " [  0   3   0  11  14]]\n",
            "\n",
            "Epoch 2/5\n",
            "Train loss: 0.8075\n",
            "Val loss:   0.9446\n",
            "Val acc:    0.7842\n",
            "Val auc:    0.9262523889070806\n",
            "Val macroF1:0.6306\n",
            "Val recalls: [0.99418605 0.7        0.63461538 0.40909091 0.46428571]\n",
            "Val CM:\n",
            " [[171   1   0   0   0]\n",
            " [  3  28   7   1   1]\n",
            " [  3  16  66  15   4]\n",
            " [  0   2   6   9   5]\n",
            " [  0   3   7   5  13]]\n",
            "\n",
            "Epoch 3/5\n",
            "Train loss: 0.6774\n",
            "Val loss:   1.0241\n",
            "Val acc:    0.7596\n",
            "Val auc:    0.9269424680646065\n",
            "Val macroF1:0.6188\n",
            "Val recalls: [0.99418605 0.775      0.48076923 0.31818182 0.67857143]\n",
            "Val CM:\n",
            " [[171   1   0   0   0]\n",
            " [  3  31   2   1   3]\n",
            " [  4  30  50   2  18]\n",
            " [  0   1   5   7   9]\n",
            " [  0   2   7   0  19]]\n",
            "\n",
            "Epoch 4/5\n",
            "Train loss: 0.6003\n",
            "Val loss:   1.1229\n",
            "Val acc:    0.7596\n",
            "Val auc:    0.920139677202169\n",
            "Val macroF1:0.6252\n",
            "Val recalls: [0.97093023 0.725      0.5        0.5        0.67857143]\n",
            "Val CM:\n",
            " [[167   4   0   0   1]\n",
            " [  0  29   6   2   3]\n",
            " [  2  16  52  17  17]\n",
            " [  0   2   0  11   9]\n",
            " [  0   1   5   3  19]]\n",
            "\n",
            "Epoch 5/5\n",
            "Train loss: 0.5128\n",
            "Val loss:   0.9331\n",
            "Val acc:    0.7978\n",
            "Val auc:    0.9269000055403914\n",
            "Val macroF1:0.6721\n",
            "Val recalls: [0.97674419 0.575      0.65384615 0.72727273 0.60714286]\n",
            "Val CM:\n",
            " [[168   3   1   0   0]\n",
            " [  0  23  12   1   4]\n",
            " [  1   4  68  18  13]\n",
            " [  0   0   2  16   4]\n",
            " [  0   1   2   8  17]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/01/18 13:01:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2026/01/18 13:01:54 WARNING mlflow.utils.requirements_utils: Found torch version (2.9.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.9.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2026/01/18 13:02:10 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.24.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torchvision==0.24.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Check MLflow runs (mlruns folder or MLflow UI).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fs0zUBAKAevr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}